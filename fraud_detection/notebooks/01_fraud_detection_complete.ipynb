{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîç Detec√ß√£o de Fraude em Cart√µes de Cr√©dito\n",
    "\n",
    "## Projeto Completo de Machine Learning\n",
    "\n",
    "Este notebook implementa um sistema de detec√ß√£o de fraude utilizando:\n",
    "- **Modelos Supervisionados**: Logistic Regression, Random Forest, XGBoost, LightGBM\n",
    "- **Detec√ß√£o de Anomalias**: Isolation Forest\n",
    "- **Abordagem H√≠brida**: Combina√ß√£o de modelos para m√°xima cobertura\n",
    "\n",
    "### Dataset\n",
    "Credit Card Fraud Detection (Kaggle) - 284.807 transa√ß√µes com 492 fraudes (~0.17%)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup e Configura√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instala√ß√£o de depend√™ncias (executar se necess√°rio)\n",
    "# !pip install pandas numpy scikit-learn xgboost lightgbm imbalanced-learn matplotlib seaborn joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Adicionar src ao path\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "if str(PROJECT_ROOT / 'src') not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT / 'src'))\n",
    "\n",
    "# Imports padr√£o\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Imports do projeto\n",
    "from data import (\n",
    "    load_credit_card_data, get_data_summary, print_data_summary,\n",
    "    split_features_target, create_time_features, create_amount_features\n",
    ")\n",
    "from features import (\n",
    "    FeatureEngineer, prepare_train_test_split, handle_imbalance,\n",
    "    create_interaction_features, get_feature_statistics\n",
    ")\n",
    "from models import (\n",
    "    FraudClassifier, train_multiple_models, hyperparameter_tuning,\n",
    "    get_model_summary, AnomalyDetector, HybridFraudDetector, train_anomaly_detector\n",
    ")\n",
    "from evaluation import (\n",
    "    calculate_metrics, print_evaluation_report, compare_models,\n",
    "    find_optimal_threshold, calculate_business_metrics, ModelEvaluator,\n",
    "    plot_class_distribution, plot_feature_distributions, plot_confusion_matrix,\n",
    "    plot_roc_curves, plot_precision_recall_curves, plot_feature_importance,\n",
    "    plot_model_comparison, plot_threshold_analysis\n",
    ")\n",
    "from utils import set_seed, print_section, Timer, print_dependencies_status\n",
    "\n",
    "# Configura√ß√µes\n",
    "set_seed(42)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "print(\"‚úì Setup conclu√≠do!\")\n",
    "print_dependencies_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Carregamento dos Dados\n",
    "\n",
    "### Download do Dataset\n",
    "Baixe o dataset do Kaggle: [Credit Card Fraud Detection](https://www.kaggle.com/mlg-ulb/creditcardfraud)\n",
    "\n",
    "Coloque o arquivo `creditcard.csv` na pasta `data/raw/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar diret√≥rios se n√£o existirem\n",
    "DATA_DIR = PROJECT_ROOT / 'data' / 'raw'\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATA_PATH = DATA_DIR / 'creditcard.csv'\n",
    "\n",
    "# Verificar se o dataset existe\n",
    "if not DATA_PATH.exists():\n",
    "    print(f\"‚ö†Ô∏è  Dataset n√£o encontrado em: {DATA_PATH}\")\n",
    "    print(\"\\nBaixe o dataset do Kaggle:\")\n",
    "    print(\"https://www.kaggle.com/mlg-ulb/creditcardfraud\")\n",
    "    print(f\"\\nColoque o arquivo creditcard.csv em: {DATA_DIR}\")\n",
    "else:\n",
    "    print(f\"‚úì Dataset encontrado: {DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar dados\n",
    "with Timer(\"Carregamento dos dados\"):\n",
    "    df = load_credit_card_data(DATA_PATH)\n",
    "\n",
    "# Resumo do dataset\n",
    "summary = get_data_summary(df)\n",
    "print_data_summary(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar primeiras linhas\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informa√ß√µes do DataFrame\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estat√≠sticas descritivas\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. An√°lise Explorat√≥ria dos Dados (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"AN√ÅLISE EXPLORAT√ìRIA DOS DADOS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Distribui√ß√£o das Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_class_distribution(df['Class'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observa√ß√£o**: O dataset √© extremamente desbalanceado (0.17% de fraudes). Isso requer t√©cnicas especiais de balanceamento e m√©tricas apropriadas para avalia√ß√£o."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 An√°lise da Vari√°vel Amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Distribui√ß√£o de Amount por classe\n",
    "for label, color, name in [(0, '#2ecc71', 'Normal'), (1, '#e74c3c', 'Fraude')]:\n",
    "    data = df[df['Class'] == label]['Amount']\n",
    "    axes[0].hist(data, bins=50, alpha=0.6, color=color, label=name, density=True)\n",
    "\n",
    "axes[0].set_xlabel('Amount')\n",
    "axes[0].set_ylabel('Densidade')\n",
    "axes[0].set_title('Distribui√ß√£o de Amount por Classe')\n",
    "axes[0].legend()\n",
    "axes[0].set_xlim(0, 500)  # Limitar para melhor visualiza√ß√£o\n",
    "\n",
    "# Boxplot\n",
    "df.boxplot(column='Amount', by='Class', ax=axes[1])\n",
    "axes[1].set_title('Amount por Classe')\n",
    "axes[1].set_xlabel('Classe (0=Normal, 1=Fraude)')\n",
    "axes[1].set_ylabel('Amount')\n",
    "plt.suptitle('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Estat√≠sticas\n",
    "print(\"\\nEstat√≠sticas de Amount por Classe:\")\n",
    "print(df.groupby('Class')['Amount'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 An√°lise Temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Distribui√ß√£o de Time por classe\n",
    "for label, color, name in [(0, '#2ecc71', 'Normal'), (1, '#e74c3c', 'Fraude')]:\n",
    "    data = df[df['Class'] == label]['Time']\n",
    "    axes[0].hist(data, bins=50, alpha=0.6, color=color, label=name, density=True)\n",
    "\n",
    "axes[0].set_xlabel('Time (segundos)')\n",
    "axes[0].set_ylabel('Densidade')\n",
    "axes[0].set_title('Distribui√ß√£o Temporal das Transa√ß√µes')\n",
    "axes[0].legend()\n",
    "\n",
    "# Converter para horas e ver padr√£o\n",
    "df['Hour'] = (df['Time'] / 3600) % 24\n",
    "fraud_by_hour = df.groupby(['Hour', 'Class']).size().unstack(fill_value=0)\n",
    "fraud_by_hour['fraud_rate'] = fraud_by_hour[1] / (fraud_by_hour[0] + fraud_by_hour[1]) * 100\n",
    "\n",
    "axes[1].plot(fraud_by_hour.index, fraud_by_hour['fraud_rate'], 'r-', linewidth=2)\n",
    "axes[1].set_xlabel('Hora do Dia')\n",
    "axes[1].set_ylabel('Taxa de Fraude (%)')\n",
    "axes[1].set_title('Taxa de Fraude por Hora')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 An√°lise das Features PCA (V1-V28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular estat√≠sticas por classe para cada feature\n",
    "pca_features = [f'V{i}' for i in range(1, 29)]\n",
    "X, y = split_features_target(df)\n",
    "feature_stats = get_feature_statistics(X[pca_features], y)\n",
    "\n",
    "print(\"Top 10 Features com maior diferen√ßa entre classes:\")\n",
    "feature_stats.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotar as 8 features mais discriminativas\n",
    "top_features = feature_stats.head(8)['feature'].tolist()\n",
    "fig = plot_feature_distributions(df, top_features, n_cols=4, figsize=(16, 8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insights da EDA:**\n",
    "1. Dataset extremamente desbalanceado (0.17% fraudes)\n",
    "2. Fraudes t√™m valores m√©dios mais baixos que transa√ß√µes normais\n",
    "3. Algumas features PCA (V14, V12, V10, V17) mostram clara separa√ß√£o entre classes\n",
    "4. Padr√µes temporais podem existir, mas s√£o sutis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Engenharia de Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"ENGENHARIA DE FEATURES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar features temporais\n",
    "df_processed = create_time_features(df)\n",
    "print(\"‚úì Features temporais criadas: Hour, Hour_sin, Hour_cos, Is_Night\")\n",
    "\n",
    "# Criar features de valor\n",
    "df_processed = create_amount_features(df_processed)\n",
    "print(\"‚úì Features de valor criadas: Amount_log, Amount_category, Is_High_Amount\")\n",
    "\n",
    "# Criar features de intera√ß√£o\n",
    "df_processed = create_interaction_features(df_processed)\n",
    "print(\"‚úì Features de intera√ß√£o criadas: V1_V2, V3_V4, PCA_magnitude\")\n",
    "\n",
    "print(f\"\\nTotal de features: {df_processed.shape[1] - 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar features e target\n",
    "X, y = split_features_target(df_processed, drop_cols=['Time', 'Hour'])\n",
    "print(f\"Shape X: {X.shape}\")\n",
    "print(f\"Shape y: {y.shape}\")\n",
    "print(f\"\\nFeatures: {list(X.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir em treino e teste\n",
    "X_train, X_test, y_train, y_test = prepare_train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=True\n",
    ")\n",
    "\n",
    "print(f\"Conjunto de treino: {X_train.shape[0]:,} amostras\")\n",
    "print(f\"Conjunto de teste: {X_test.shape[0]:,} amostras\")\n",
    "print(f\"\\nDistribui√ß√£o no treino: {dict(y_train.value_counts())}\")\n",
    "print(f\"Distribui√ß√£o no teste: {dict(y_test.value_counts())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalar features\n",
    "feature_engineer = FeatureEngineer(scaler_type='robust')\n",
    "X_train_scaled = feature_engineer.fit_transform(X_train)\n",
    "X_test_scaled = feature_engineer.transform(X_test)\n",
    "\n",
    "print(\"‚úì Features escaladas com RobustScaler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Treinamento dos Modelos Supervisionados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"TREINAMENTO DOS MODELOS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Configura√ß√£o dos Modelos\n",
    "\n",
    "Vamos treinar e comparar 4 modelos:\n",
    "\n",
    "1. **Logistic Regression**: Baseline simples e interpret√°vel\n",
    "2. **Random Forest**: Ensemble de √°rvores, robusto a overfitting\n",
    "3. **XGBoost**: Gradient boosting otimizado, excelente para dados tabulares\n",
    "4. **LightGBM**: Gradient boosting eficiente, bom para grandes datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura√ß√£o dos modelos\n",
    "# Nota: scale_pos_weight para XGBoost √© calculado como ratio das classes\n",
    "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "print(f\"Scale pos weight (XGBoost): {scale_pos_weight:.2f}\")\n",
    "\n",
    "model_configs = {\n",
    "    'Logistic Regression': {\n",
    "        'type': 'logistic_regression',\n",
    "        'params': {\n",
    "            'max_iter': 1000,\n",
    "            'class_weight': 'balanced',\n",
    "            'random_state': 42,\n",
    "            'solver': 'lbfgs'\n",
    "        }\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'type': 'random_forest',\n",
    "        'params': {\n",
    "            'n_estimators': 100,\n",
    "            'max_depth': 10,\n",
    "            'class_weight': 'balanced',\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'type': 'xgboost',\n",
    "        'params': {\n",
    "            'n_estimators': 100,\n",
    "            'max_depth': 5,\n",
    "            'learning_rate': 0.1,\n",
    "            'scale_pos_weight': scale_pos_weight,\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1,\n",
    "            'eval_metric': 'aucpr',\n",
    "            'use_label_encoder': False\n",
    "        }\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'type': 'lightgbm',\n",
    "        'params': {\n",
    "            'n_estimators': 100,\n",
    "            'max_depth': 5,\n",
    "            'learning_rate': 0.1,\n",
    "            'class_weight': 'balanced',\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1,\n",
    "            'verbose': -1\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar todos os modelos\n",
    "with Timer(\"Treinamento de todos os modelos\"):\n",
    "    models = train_multiple_models(X_train_scaled, y_train, model_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumo dos modelos treinados\n",
    "model_summary = get_model_summary(models)\n",
    "model_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Avalia√ß√£o dos Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"AVALIA√á√ÉO DOS MODELOS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar avaliador\n",
    "evaluator = ModelEvaluator(y_test.values, amounts=X_test['Amount'].values)\n",
    "\n",
    "# Avaliar cada modelo\n",
    "results = {}\n",
    "predictions = {}\n",
    "probabilities = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    predictions[name] = y_pred\n",
    "    probabilities[name] = y_proba\n",
    "    \n",
    "    results[name] = evaluator.evaluate_model(name, y_pred, y_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabela comparativa\n",
    "print_section(\"COMPARA√á√ÉO DE MODELOS\")\n",
    "\n",
    "comparison_df = evaluator.get_comparison_table()\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Visualiza√ß√£o das Curvas ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_roc_curves(y_test.values, probabilities)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Visualiza√ß√£o das Curvas Precision-Recall\n",
    "\n",
    "**Importante**: Para datasets desbalanceados, a curva PR √© mais informativa que ROC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_precision_recall_curves(y_test.values, probabilities)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Compara√ß√£o Visual dos Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_model_comparison(\n",
    "    comparison_df, \n",
    "    metrics=['precision', 'recall', 'f1', 'average_precision']\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Matriz de Confus√£o do Melhor Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar melhor modelo por Average Precision\n",
    "best_model_name = comparison_df['average_precision'].idxmax()\n",
    "print(f\"Melhor modelo (Average Precision): {best_model_name}\")\n",
    "\n",
    "fig = plot_confusion_matrix(\n",
    "    y_test.values, \n",
    "    predictions[best_model_name],\n",
    "    title=f\"Matriz de Confus√£o - {best_model_name}\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Import√¢ncia das Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import√¢ncia das features do melhor modelo baseado em √°rvores\n",
    "for model_name in ['XGBoost', 'Random Forest', 'LightGBM']:\n",
    "    if model_name in models:\n",
    "        importance = models[model_name].get_feature_importance()\n",
    "        if importance:\n",
    "            fig = plot_feature_importance(\n",
    "                importance, \n",
    "                top_n=15,\n",
    "                title=f\"Import√¢ncia das Features - {model_name}\"\n",
    "            )\n",
    "            plt.show()\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. An√°lise de Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"AN√ÅLISE DE THRESHOLD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise de threshold para o melhor modelo\n",
    "best_proba = probabilities[best_model_name]\n",
    "\n",
    "fig = plot_threshold_analysis(y_test.values, best_proba)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encontrar threshold √≥timo\n",
    "optimal_threshold, optimal_f1 = find_optimal_threshold(y_test.values, best_proba, metric='f1')\n",
    "print(f\"Threshold √≥timo para F1: {optimal_threshold:.2f}\")\n",
    "print(f\"F1-Score no threshold √≥timo: {optimal_f1:.4f}\")\n",
    "\n",
    "# Comparar com threshold padr√£o\n",
    "y_pred_default = (best_proba >= 0.5).astype(int)\n",
    "y_pred_optimal = (best_proba >= optimal_threshold).astype(int)\n",
    "\n",
    "print(\"\\nCompara√ß√£o de resultados:\")\n",
    "print(f\"\\nThreshold 0.5:\")\n",
    "metrics_default = calculate_metrics(y_test.values, y_pred_default, best_proba)\n",
    "print(f\"  Precision: {metrics_default['precision']:.4f}\")\n",
    "print(f\"  Recall: {metrics_default['recall']:.4f}\")\n",
    "print(f\"  F1: {metrics_default['f1']:.4f}\")\n",
    "\n",
    "print(f\"\\nThreshold {optimal_threshold:.2f}:\")\n",
    "metrics_optimal = calculate_metrics(y_test.values, y_pred_optimal, best_proba)\n",
    "print(f\"  Precision: {metrics_optimal['precision']:.4f}\")\n",
    "print(f\"  Recall: {metrics_optimal['recall']:.4f}\")\n",
    "print(f\"  F1: {metrics_optimal['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Detec√ß√£o de Anomalias (N√£o Supervisionado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"DETEC√á√ÉO DE ANOMALIAS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Isolation Forest\n",
    "\n",
    "O Isolation Forest aprende o comportamento \"normal\" e detecta outliers. √â √∫til para identificar novos padr√µes de fraude n√£o vistos no treinamento supervisionado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar Isolation Forest apenas com dados normais\n",
    "anomaly_detector = train_anomaly_detector(\n",
    "    X_train_scaled, \n",
    "    y_train,\n",
    "    model_type='isolation_forest',\n",
    "    params={\n",
    "        'n_estimators': 100,\n",
    "        'contamination': 0.001,  # ~0.1% de anomalias esperadas\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1\n",
    "    },\n",
    "    train_on_normal_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliar detector de anomalias\n",
    "anomaly_predictions = anomaly_detector.predict(X_test_scaled)\n",
    "# Converter: -1 (anomalia) -> 1 (fraude), 1 (normal) -> 0\n",
    "anomaly_as_fraud = (anomaly_predictions == -1).astype(int)\n",
    "\n",
    "print(\"Avalia√ß√£o do Isolation Forest:\")\n",
    "anomaly_metrics = print_evaluation_report(\n",
    "    y_test.values, \n",
    "    anomaly_as_fraud, \n",
    "    anomaly_detector.predict_proba_anomaly(X_test_scaled),\n",
    "    \"Isolation Forest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Sistema H√≠brido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"SISTEMA H√çBRIDO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O sistema h√≠brido combina:\n",
    "- **Modelo Supervisionado**: Detecta padr√µes conhecidos de fraude\n",
    "- **Detector de Anomalias**: Identifica comportamentos at√≠picos (possivelmente novas fraudes)\n",
    "\n",
    "Uma transa√ß√£o √© marcada como suspeita se qualquer um dos m√©todos indicar alto risco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar detector h√≠brido\n",
    "best_supervised_model = models[best_model_name]\n",
    "\n",
    "hybrid_detector = HybridFraudDetector(\n",
    "    supervised_model=best_supervised_model,\n",
    "    anomaly_detector=anomaly_detector,\n",
    "    supervised_weight=0.7,  # 70% peso para modelo supervisionado\n",
    "    anomaly_weight=0.3      # 30% peso para detector de anomalias\n",
    ")\n",
    "\n",
    "print(f\"Sistema h√≠brido criado com:\")\n",
    "print(f\"  - Modelo supervisionado: {best_model_name} (peso: 70%)\")\n",
    "print(f\"  - Detector de anomalias: Isolation Forest (peso: 30%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliar sistema h√≠brido\n",
    "hybrid_proba = hybrid_detector.predict_proba(X_test_scaled)\n",
    "hybrid_pred = hybrid_detector.predict(X_test_scaled, threshold=0.5)\n",
    "\n",
    "print(\"Avalia√ß√£o do Sistema H√≠brido:\")\n",
    "hybrid_metrics = print_evaluation_report(\n",
    "    y_test.values, \n",
    "    hybrid_pred, \n",
    "    hybrid_proba,\n",
    "    \"Sistema H√≠brido\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise detalhada do sistema h√≠brido\n",
    "breakdown = hybrid_detector.get_detection_breakdown(X_test_scaled)\n",
    "breakdown_fraud = breakdown[y_test.values == 1]\n",
    "\n",
    "print(\"\\nAn√°lise das fraudes detectadas:\")\n",
    "print(f\"Total de fraudes: {len(breakdown_fraud)}\")\n",
    "print(f\"\\nFraudes detectadas pelo supervisionado (proba > 0.5): {(breakdown_fraud['supervised_proba'] > 0.5).sum()}\")\n",
    "print(f\"Fraudes detectadas pelo detector de anomalias (proba > 0.5): {(breakdown_fraud['anomaly_proba'] > 0.5).sum()}\")\n",
    "print(f\"Fraudes detectadas pelo sistema h√≠brido: {breakdown_fraud['final_prediction'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Compara√ß√£o Final e Conclus√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"COMPARA√á√ÉO FINAL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicionar resultados do Isolation Forest e Sistema H√≠brido √† compara√ß√£o\n",
    "all_results = results.copy()\n",
    "all_results['Isolation Forest'] = anomaly_metrics\n",
    "all_results['Sistema H√≠brido'] = hybrid_metrics\n",
    "\n",
    "# Criar DataFrame de compara√ß√£o\n",
    "final_comparison = compare_models(all_results)\n",
    "final_comparison = final_comparison.sort_values('average_precision', ascending=False)\n",
    "\n",
    "print(\"Ranking dos Modelos (por Average Precision):\")\n",
    "final_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza√ß√£o final\n",
    "fig = plot_model_comparison(\n",
    "    final_comparison, \n",
    "    metrics=['precision', 'recall', 'f1', 'average_precision'],\n",
    "    figsize=(14, 6)\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curvas ROC e PR incluindo todos os modelos\n",
    "all_probabilities = probabilities.copy()\n",
    "all_probabilities['Isolation Forest'] = anomaly_detector.predict_proba_anomaly(X_test_scaled)\n",
    "all_probabilities['Sistema H√≠brido'] = hybrid_proba\n",
    "\n",
    "fig = plot_precision_recall_curves(y_test.values, all_probabilities, figsize=(12, 8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Conclus√µes e Recomenda√ß√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"CONCLUS√ïES E RECOMENDA√á√ïES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar melhor modelo\n",
    "best_overall = final_comparison['average_precision'].idxmax()\n",
    "best_recall = final_comparison['recall'].idxmax()\n",
    "best_precision = final_comparison['precision'].idxmax()\n",
    "\n",
    "print(\"RESULTADOS FINAIS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n1. MELHOR MODELO GERAL (Average Precision): {best_overall}\")\n",
    "print(f\"   - Average Precision: {final_comparison.loc[best_overall, 'average_precision']:.4f}\")\n",
    "print(f\"   - F1-Score: {final_comparison.loc[best_overall, 'f1']:.4f}\")\n",
    "\n",
    "print(f\"\\n2. MELHOR RECALL (detectar mais fraudes): {best_recall}\")\n",
    "print(f\"   - Recall: {final_comparison.loc[best_recall, 'recall']:.4f}\")\n",
    "\n",
    "print(f\"\\n3. MELHOR PRECISION (menos falsos positivos): {best_precision}\")\n",
    "print(f\"   - Precision: {final_comparison.loc[best_precision, 'precision']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RECOMENDA√á√ïES\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "1. PRODU√á√ÉO:\n",
    "   - Use o modelo com melhor Average Precision como modelo principal\n",
    "   - Ajuste o threshold conforme a toler√¢ncia a falsos positivos\n",
    "   - Considere o Sistema H√≠brido para detectar fraudes emergentes\n",
    "\n",
    "2. MONITORAMENTO:\n",
    "   - Implemente monitoramento cont√≠nuo das m√©tricas\n",
    "   - Retreine periodicamente com novos dados\n",
    "   - Monitore drift nos padr√µes de fraude\n",
    "\n",
    "3. MELHORIAS FUTURAS:\n",
    "   - Adicionar mais features de comportamento do usu√°rio\n",
    "   - Implementar modelo de s√©ries temporais para padr√µes sequenciais\n",
    "   - Considerar ensemble de m√∫ltiplos modelos\n",
    "   - Testar deep learning (autoencoders) para detec√ß√£o de anomalias\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Salvar Modelos para Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar diret√≥rio para modelos\n",
    "MODELS_DIR = PROJECT_ROOT / 'models_saved'\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Salvar melhor modelo supervisionado\n",
    "best_supervised_model.save(MODELS_DIR / f'{best_model_name.lower().replace(\" \", \"_\")}_model.pkl')\n",
    "\n",
    "# Salvar detector de anomalias\n",
    "anomaly_detector.save(MODELS_DIR / 'isolation_forest_detector.pkl')\n",
    "\n",
    "# Salvar feature engineer (scaler)\n",
    "import joblib\n",
    "joblib.dump(feature_engineer, MODELS_DIR / 'feature_engineer.pkl')\n",
    "\n",
    "print(f\"\\n‚úì Modelos salvos em: {MODELS_DIR}\")\n",
    "print(\"\\nArquivos:\")\n",
    "for f in MODELS_DIR.glob('*.pkl'):\n",
    "    print(f\"  - {f.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Exemplo de Uso em Produ√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_fraud(transaction_data: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Fun√ß√£o para predi√ß√£o de fraude em produ√ß√£o.\n",
    "    \n",
    "    Args:\n",
    "        transaction_data: DataFrame com dados da transa√ß√£o\n",
    "        \n",
    "    Returns:\n",
    "        Dicion√°rio com predi√ß√£o e probabilidade\n",
    "    \"\"\"\n",
    "    # Carregar modelos\n",
    "    model = FraudClassifier.load(MODELS_DIR / f'{best_model_name.lower().replace(\" \", \"_\")}_model.pkl')\n",
    "    scaler = joblib.load(MODELS_DIR / 'feature_engineer.pkl')\n",
    "    \n",
    "    # Processar features\n",
    "    X = scaler.transform(transaction_data)\n",
    "    \n",
    "    # Predi√ß√£o\n",
    "    proba = model.predict_proba(X)[:, 1]\n",
    "    pred = (proba >= 0.5).astype(int)\n",
    "    \n",
    "    return {\n",
    "        'is_fraud': bool(pred[0]),\n",
    "        'fraud_probability': float(proba[0]),\n",
    "        'risk_level': 'HIGH' if proba[0] > 0.7 else 'MEDIUM' if proba[0] > 0.3 else 'LOW'\n",
    "    }\n",
    "\n",
    "# Exemplo de uso\n",
    "sample_transaction = X_test.iloc[[0]]\n",
    "result = predict_fraud(sample_transaction)\n",
    "print(\"Exemplo de predi√ß√£o:\")\n",
    "print(f\"  √â fraude: {result['is_fraud']}\")\n",
    "print(f\"  Probabilidade: {result['fraud_probability']:.4f}\")\n",
    "print(f\"  N√≠vel de risco: {result['risk_level']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Resumo do Projeto\n",
    "\n",
    "Este notebook implementou um sistema completo de detec√ß√£o de fraude com:\n",
    "\n",
    "1. **EDA**: An√°lise explorat√≥ria identificando o severo desbalanceamento e padr√µes nas features\n",
    "2. **Feature Engineering**: Cria√ß√£o de features temporais, de valor e intera√ß√µes\n",
    "3. **Modelos Supervisionados**: Treinamento e compara√ß√£o de 4 algoritmos\n",
    "4. **Detec√ß√£o de Anomalias**: Isolation Forest para fraudes emergentes\n",
    "5. **Sistema H√≠brido**: Combina√ß√£o de abordagens para m√°xima cobertura\n",
    "6. **Avalia√ß√£o**: M√©tricas apropriadas para dados desbalanceados\n",
    "7. **Deploy**: Salvamento de modelos prontos para produ√ß√£o\n",
    "\n",
    "### Pr√≥ximos Passos\n",
    "- Implementar API REST para servir predi√ß√µes\n",
    "- Criar dashboard de monitoramento\n",
    "- Configurar retreinamento autom√°tico"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
